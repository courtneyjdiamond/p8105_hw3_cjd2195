---
title: "P8105 Homework 3"
author: "Courtney Diamond"
output: github_document
date: "2023-10-14"
---

Let's start by loading our libraries and creating some default settings. 

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(magrittr)
library(ggridges)
library(lubridate)


knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)


theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Question 1

Great. Let's load our first dataset. 

```{r}
data("instacart")

instacart = 
  instacart |>
  as_tibble()
```

We are looking at a dataset of `r nrow(instacart)` observations and `r ncol(instacart)` variables, all relating to orders from Instacart. The variables include:

* `order_id`: the unique identifier of the order
* `product_id`: the unique identifier of the product
* `add_to_cart_order`: the order in which the item was placed into the cart
* `reordered`: an indicator of whether or not the product is a repeat order
* `user_id`: a user's unique identifier
* `eval_set`: whether the observation is part of a training or test set for some model
* `order_number`: for a specific user, the sequence of the whole order in relation to all orders made
* `order_dow`: which day of the week the order was placed
* `order_hour_of_day`: which hour of the day the order was placed
* `days_since_prior_order`: number of days since the user's last order
* `product_name`: the human-readable product name
* `aisle_id`: the unique identifier of the aisle the product is in
* `department_id`: the unique identifier of the department the product is in
* `aisle`: the human-readable name of the aisle the product is in
* `department`: the human-readable name of the department the product is in

Each individual line or observations corresponds to a specific product in a specific order by a specific user, and contains additional details about the order and product. Let's walk through the first line. `order_id` is 1; this represents a single order by a single user, and is the only order in the dataset with this ID. `product_id` is the product in question; the corresponding human-readable name is in the `product_name` column. The `add_to_cart_order` tells us that this product was the very first thing the person put in their cart when building this order, and `reordered` tells us whether or not the person ordered the product before (in this case, yes). The `user_id` is 112108; all of this person's orders and products will have this value. `eval_set` doesn't give us information about the order or product, but rather what the downstream use of the observation is for: in this case, it will be part of the end-user's training set for a given ML or other model. `order_number` tells us that this is the fourth order for this individual user, and `order_dow` tells us which day of the week the order was placed on (but there's no data to tell us which day the 4th day of the week is, so it's a little hard to interpret). `order_hour_of_day` tells us they ordered this sometime between 10AM and 11AM (presuming they're using standard military time). We can see that there have been 9 days since the user's last order, as indicated by the `days_since_prior_order` column. We've already covered that the `product_name` is the human readable version of the `product_id`, so in this case we're looking at some Bulgarian yogurt; `aisle_id` and `department_id` tell us that this is found in a specific aisle and department; the corresponding `aisle` and `department` variables tell us in human-readable terms that we can find this product in the yogurt aisle of the "dairy eggs" department. 

Let's do some tidying! I don't think we need the `eval_set` variable (the online description notes we are only working with training observations here, and none of the questions I have pertain to this). I'm also going to reorder so similar variables are grouped together, and recode the `reordered` variable to "yes" and "no" to make it more obvious to a human reader. I'll hold off on recoding the `order_dow` variable because there's no definitive answer in the original README file online about which number corresponds to which day. 

```{r}
tidy_instacart = 
instacart |> 
  select(!eval_set) |> 
  relocate(order_id,
           user_id,
           order_number,
           order_dow,
           order_hour_of_day,
           days_since_prior_order,
           add_to_cart_order,
           reordered,
           product_id,
           product_name,
           aisle_id, aisle,
           department_id,
           department) |> 
  mutate(
    reordered = 
      case_match(
        reordered,
        1 ~ "yes",
        0 ~ "no"), 
    reordered = 
      as.factor(reordered))

```

First up, we want to know how many aisles there are. We can establish this by calling `group_by(aisle_id)`, and figuring out how many groups are consequently made. We end up with `r tidy_instacart |> group_by(aisle_id) |> n_groups()` aisles. 

```{r}
tidy_instacart |> 
  group_by(aisle_id) |> 
  n_groups()
```

We also want to know which aisles have the most products ordered from them! I am going to interpret this as raw counts represented by the number of times a product is listed in the table, and NOT group by the item itself as a conceptual level. (That is, if there are 1,000,000 rows corresponding to 1 type of ice cream, I will count each of those individually, and not simply as 1 product.) We end up with the `r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(1) %$% aisle` aisle having the most products ordered (`r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(1) %$% n`), followed by the `r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(2) %$% aisle` with `r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(2) %$% n` products, and the `r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(3) %$% aisle` taking third place with `r tidy_instacart |> group_by(aisle_id, aisle) |>  summarize(n = n()) |> arrange(desc(n)) |> ungroup(aisle_id) |> slice(3) %$% n` products.


Great- now let's graph it!
```{r}
tidy_instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(aisle = 
           fct_reorder(aisle, n, desc)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(title = "Number of Products Ordered from Each Aisle")
```

Now we want to examine a few aisles more closely. 

```{r}
tidy_instacart |> 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") |> 
  count(aisle, product_name) |> 
  group_by(aisle) |> 
  arrange(desc(n)) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  select(!rank) |>  
  knitr::kable()
```


Now let's look at a couple of specific products. 

```{r}
tidy_instacart |> 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") |> 
  group_by(product_name, order_dow) |> 
  summarize(mean_hour = mean(order_hour_of_day)) |> 
  pivot_wider(names_from = order_dow,
              values_from = mean_hour) |> 
  knitr::kable()
```


## Question 2

First up: loading and cleaning data!

```{r}
data("brfss_smart2010")

brfss_df = brfss_smart2010 |> 
  as_tibble()

tidy_brfss = 
  brfss_df |> 
  janitor::clean_names() |> 
  rename(location_abbr = locationabbr,
         location_desc = locationdesc,
         ) |> 
  filter(topic == "Overall Health") |> 
  mutate(response =
           forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```


### Question 2A

In 2002, only `r tidy_brfss |> filter (year == 2002) |> count(location_abbr) |> filter(n >= (5*7)) |> nrow()` states had more than 7 locations polled. This compares to `r tidy_brfss |> filter (year == 2010) |> count(location_abbr) |> filter(n >= (5*7)) |> nrow()` states in 2010. 

To get these values, I filtered first to the year desired, then got the counts of all the occurrences of a given state `location_abbr`. Because, for this specific question, there will always be five observations for the same site (one corresponding to each of the potential responses), I need to account for this when filtering by counts; I adjust the desired threshold of 7 by multiplying by 5. Finally, I call `nrow()` to get the number of states that are pulled back by the above filter.

### Question 2B
```{r, fig.asp= 1}

tidy_brfss |> 
  filter(response == "Excellent") |> 
  group_by(year, location_abbr) |> 
  summarize(n = n(), average_state_prevalence = mean(data_value)) |> 
  ggplot(aes(x = year, y = average_state_prevalence, color = fct_reorder(location_abbr, average_state_prevalence))) +
  geom_line() + 
  labs(
    title = "Average Percentage of Self-Reported 'Excellent' Health Status by State, 2002-2010",
    x = "Year",
    y = "Percentage Reported",
    color = "State"
  ) + 
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")

```

Spaghetti plots are very scary to look at when you want to identify the trend for an individual observation across time; however, they're great for assessing overall trends without losing the granularity of individual trends. For this question, I originally did not refactor any variables and got a plot that was overwhelming to look at; while the legend was in alphabetical order by state, trying to find an individual state by its own shade on the color palate is nearly impossible because the shades are so close to one another. Instead, I decided to refactor the variable corresponding to state (`location_abbr`) based on the value of the `average_state_prevalence`, which now organizes my color palate with respect to the percentage we're interested in learning. It's easier for me to see that DC is one of the yellow lines, and therefore one of the locations reporting the highest rates of "Excellent" health, and WV is one of the dark purple lines, and therefore has one of the lowest reported rates. In this case, the legend is almost more informative than the graph itself.

### Question 2C
```{r}
tidy_brfss |> 
  filter(year == 2006 | year == 2010) |> 
  filter(location_abbr == "NY") |> 
  group_by(year, response) |> 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .5) +
  facet_grid(~year) + 
  labs(
    title = "Distributions of Overall Health Responses Among NY Locations, 2006 and 2010",
    x = "Reported Percentage",
    y = "Density",
    fill = "Response"
  ) +
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")

tidy_brfss |> 
  filter(year == 2006 | year == 2010) |> 
  filter(location_abbr == "NY") |> 
  group_by(year, response) |> 
  ggplot(aes(x = data_value, y = response)) +
  geom_density_ridges(aes(fill = response)) +
  facet_grid(~year) +
  labs(
    title = "Distributions of Overall Health Responses Among NY Locations, 2006 and 2010",
    x = "Reported Percentage",
    y = "Response",
    fill = "Response"
  ) +
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")

tidy_brfss |> 
  filter(year == 2006 | year == 2010) |> 
  filter(location_abbr == "NY") |> 
  group_by(year, response) |> 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(~year) +
  labs(
    title = "Distributions of Overall Health Responses Among NY Locations, 2006 and 2010",
    x = "Response",
    y = "Reported Percentage"
  ) + 
  theme(plot.title = element_text(size = 12), plot.title.position = "plot",
        axis.text.x = element_text(angle = 60, hjust = 1))

tidy_brfss |> 
  filter(year == 2006 | year == 2010) |> 
  filter(location_abbr == "NY") |> 
  group_by(year, response) |> 
  ggplot(aes(x = response, y = data_value)) + 
  geom_violin(aes())+
  facet_grid(~year) + 
  stat_summary(fun = "mean") +
  labs(
    title = "Distributions of Overall Health Responses Among NY Locations, 2006 and 2010",
    x = "Response",
    y = "Reported Percentage"
  ) + 
  theme(plot.title = element_text(size = 12), plot.title.position = "plot",
        axis.text.x = element_text(angle = 60, hjust = 1))
```

I generated a few plots for this question, because I wanted to see which one I'd find most informative if I were just a random person interested in looking at the distribution. I think each of them has their own merit depending on what a reader is looking for from a visualization of this data. The first plot is a density plot, and provides a good "quick" visual that generally shows us how the distributions differed between the two time points, but the overlap is a little busy. The ridgeplot is a good alternative if I'm set on looking at the shape of the distributions (assessing normality, bimodal, etc.) but I don't think that's super helpful here because I'm mostly just trying to compare the different responses to one another rather than understand their individual distributions. The boxplot provides more immediately accessible information about the mean, IQR, and outliers, as does the violin plot, albeit in a more ~ aesthetically pleasing ~ kind of way. I think I personally prefer the boxplot though. 


## Question 3

Let's read, clean, tidy, and join some data! I'm going to both recode the sex and education levels, as well as relevel them as factor variables to help me later on. Then I'll join the datasets, filter by the specified age, and also drop any rows with NAs in the demographic columns. 

```{r}
nhanes_accel = read_csv("data/nhanes_accel.csv") |> 
  janitor::clean_names()


nhanes_covars = read_csv("data/nhanes_covar.csv", 
                         skip = 4) |> 
  janitor::clean_names() |> 
  mutate(sex = 
         case_match(
           sex,
           1 ~ "male",
           2 ~ "female"),
         education = 
           case_match(
             education,
             1 ~ "Less than high school",
             2 ~ "High school equivalent",
             3 ~ "More than high school"
           ), 
         sex = as.factor(sex), 
         sex = fct_relevel(sex,
                           c("male",
                             "female")),
         education = as.factor(education),
         education = fct_relevel(education,
                                 c("Less than high school",
                                 "High school equivalent",
                                 "More than high school"))
                              )

nhanes_combined = full_join(nhanes_covars, nhanes_accel, by = "seqn") |> 
  filter(age > 21) |> 
  drop_na(sex, age, bmi, education)
```

### Question 3A

```{r}
nhanes_combined |> 
  count(sex, education) |> 
  pivot_wider(names_from = sex,
              values_from = n) |> 
  knitr::kable()
```

First up is summarizing the counts of participants by age and sex. 

```{r}
nhanes_combined |> 
  group_by(sex, education) |> 
  ggplot(aes(x = education, y = age, color = sex)) + 
  geom_boxplot() +
  labs(
    title = "NHANES Accelerometer Age Distrubtions by Sex and Education Level",
    x = "Highest Education Level",
    y = "Age",
    color = "Sex"
  ) +
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")
```

Second up is making a plot to show the distribution of ages of participants by both sex and education level. I have to admit I'm not a huge fan of these two colors, but will defer to the wish to use the viridis color palette. 

Our data seem to be saying that there are roughly the same number of men and women in the study with a highest education level of "less than high-school", and their average age is roughly the same as is the range and IQR of ages. Noticeably there are more men than women with a high-school equivalent education, and the average age and IQR of age of women in this category is higher than men, though the range of ages is about the same. Finally, there are roughly similiar numbers of men and women whose education level extends beyond high school, and in this group, the men's average age is higher than the women's while the range and IQR are roughly the same.


### Question 3B

```{r}
nhanes_combined |> 
  mutate(
    total_activity_sum = 
      rowSums(
        across(
          c("min1":"min1440")
          )
        ),
    mean_activity = 
      rowMeans(
        across(
          c("min1":"min1440")
        )
      )
    ) |> 
  group_by(sex, education) |> 
  ggplot(aes(x = age, y = mean_activity, color = sex)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  facet_grid(~education) +
  labs(
    title = "Mean Activity Level vs Age by Sex and Education Level ",
    x = "Age",
    y = "Mean Activity Level (in MIMS)",
    color = "Sex"
  ) +
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")


```

There are a lot of steps involved in this part of the question! Firstly, I'm going to create two summary variables, one that sums all activity and another that averages all activity. (In retrospect I realize that the mean is sort of useless because it's just a scaled version of the sum...but it is nice because it makes the values smaller so I'm going to use it in my visualization.) Now I want to group by sex and education because I am interested in knowing how activity levels vary among these groups. Finally, time to plot!  

Our plots show some interesting trends. For the "less than high school" education level, we can see an overall decrease in measured activity levels as age increases, but both men and women see a change in the trend as they approach 60 years (men see an uptick and women see a plateau). Though women start with higher mean activity levels than men before the age of 30 in this category, both men and women ultimately reach similar levels around 80 years of age. The difference between final and initial activity levels (for both men and women) is the greatest for this educational level compared to the others. 

The second facet, the "high school equivalent" group, also shows an overall decrease in activity levels for both women and men, but the trends do not take the same shape as those in the previous group. Women see an increase between the ages of 20 and 40 years, then a decline into their 60s, followed by an uptick in their 70s before ultimatley decreasing to their lowest levels at the age of 80. Men don't have the same initial uptick as they approach their 40s, but do show the same decline into their 60s, and ultimately have similar levels to women as they approach 80.

Last but not least, the "more than high school" group shows the most stable overall trends as compared to the other groups. The initial activity levels at age 20 for both men and women are lower than those for those in the "less than high school" group, but are roughly the same as the "high school equivalent" group. However, for both men and women, they stay roughly the same to age 50; here, men show a steady decline in levels until they reach their 80s, and women continue with the same initial level until their 60s. The difference between the final and initial mean activity levels is similar to that in the high-school equivalent educaiton group. 

### Question 3C

```{r}
nhanes_combined |> 
  pivot_longer("min1":"min1440", 
               names_to = "minute",
               names_prefix = "min",
               values_to = "mims") |> 
  mutate(minute = 
           as.integer(minute)) |> 
  group_by(sex, education, minute) |> 
  summarize(mean_mims = mean(mims)) |> 
  mutate(time_of_day = 
           hms::hms(minutes = minute)) |>
  ggplot(aes(x = time_of_day, y = mean_mims, color = sex)) +
  geom_smooth(se = FALSE) + 
  facet_grid(~education) +
  labs(
    title = "24-hour Activity Levels by Sex and Education Levels",
    x = "Time of Day (hh:mm:ss)",
    y = "Mean Activity Level (in MIMS)",
    color = "Sex"
  ) +
  theme(plot.title = element_text(size = 12), plot.title.position = "plot")
```

At a 10,000 foot glance, the overall trends of 24-hour activity levels are the same among educational groups and sexes: we see a nadir just before 5:00AM, a peak sometime between 10:00AM and 8:00PM, then a decline in activity that we can presume continues until the next morning's nadir. However, there are some subtle differences to note: for the "less than high school group", there are two parts to the descending curve after reaching the peak level: a slight decrease through around 8:00PM, then a steeper decrease through midnight. The high-school equivalent and more than high school groups appear to have more prominent secondary peaks, around 8:00PM, then followed by a similar steep decrease in activity through midnight. The local minimum appears at roughly the same time for each group, 3:00PM. It appears that the high school equivalent education group has the lowest peak activity levels throughout the day, and the less than high school group the highest. 

Thus far I've touched on differences by educational level; for sex, differences are most apparent in the "more than high school" educational group. Women have higher activity levels than men, and appear to reach that peak earlier than men (slightly before 10:00AM compared to roughly 12:00noon.) Women in the high school equivalent group also have higher activity levels, but most distinctly in the morning, where they too reach their peak sooner than men. Finally, women in the less than high school group also have higher peak levels than men (though to a lesser degree), but ever so slightly lower levels during ascent and descent. They also appear to have less activity thoughout the early morning hours. 

Conlcusions I feel comfortable making, without having performed any statistical analyses: activity is lowest early in the morning, after midnight until roughly 5AM, for all educational levels. This most likely corresponds to sleep. Peak activity levels vary by both sex and education level; appear to have higher peak mean activity levels than men, and those with less than a high school educaiton have higher peaks than those in the high school or more than high school education groups. The high-school equivalent and more than high school groups appear to have a secondary peak after a small decrease after the first peak; it would be interesting to observe them and understand what the dip and peak corresponds to (maybe post-prandial sleepiness, followed by a resurgence in energy?). I'd be curious to know, more importantly, why I *don't* see this dip and secondary peak in the less than high-school group. 
